{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_example_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FCzBccRGyWP",
        "outputId": "c4ad4053-0658-407d-e4a4-da8e77d3dd36"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx9Wg-st4fXH",
        "outputId": "3f265a30-cc4f-4343-bd7d-30112356ebf5"
      },
      "source": [
        "%cd /content/drive/My Drive/github/selfiespredict\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHBPPQALPNJA",
        "outputId": "303e3bc8-41dd-4b52-8b62-f912df84e36a"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu.log  gpu_usage.sh  \u001b[0m\u001b[01;34mrun\u001b[0m/  test_model_params.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlsKLKDuBwKa",
        "outputId": "3f8d83fa-360c-46a7-fc1f-ccf1bdf1211e"
      },
      "source": [
        "!git pull "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 10 (delta 4), reused 10 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects:  10% (1/10)   \rUnpacking objects:  20% (2/10)   \rUnpacking objects:  30% (3/10)   \rUnpacking objects:  40% (4/10)   \rUnpacking objects:  50% (5/10)   \rUnpacking objects:  60% (6/10)   \rUnpacking objects:  70% (7/10)   \rUnpacking objects:  80% (8/10)   \rUnpacking objects:  90% (9/10)   \rUnpacking objects: 100% (10/10)   \rUnpacking objects: 100% (10/10), done.\n",
            "From https://github.com/bananenpampe/selfiespredict\n",
            "   96c9a51..20e03c9  main       -> origin/main\n",
            "Updating 96c9a51..20e03c9\n",
            "Fast-forward\n",
            " run/testmodel/run.yaml               | 23 \u001b[32m+++++++++++++++++++++++\u001b[m\n",
            " run/testmodel/test_model_params.yaml |  1 \u001b[32m+\u001b[m\n",
            " 2 files changed, 24 insertions(+)\n",
            " create mode 100644 run/testmodel/run.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLmn_Th6N74Z"
      },
      "source": [
        "! pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-WK8EMl6-qc",
        "outputId": "48592d94-9043-4512-aba0-beb92a1cf164"
      },
      "source": [
        "! pip install -e ."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/My%20Drive/github/selfiespredict\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from selfiespredict==0.0.post1.dev46+g2703e11) (4.8.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from selfiespredict==0.0.post1.dev46+g2703e11) (7.1.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from selfiespredict==0.0.post1.dev46+g2703e11) (3.6.4)\n",
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting selfies\n",
            "  Downloading selfies-2.0.0-py3-none-any.whl (33 kB)\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2021.9.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown->selfiespredict==0.0.post1.dev46+g2703e11) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown->selfiespredict==0.0.post1.dev46+g2703e11) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown->selfiespredict==0.0.post1.dev46+g2703e11) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->selfiespredict==0.0.post1.dev46+g2703e11) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->selfiespredict==0.0.post1.dev46+g2703e11) (3.10.0.2)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (2.7.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.1.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (3.13)\n",
            "Collecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.42.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->selfiespredict==0.0.post1.dev46+g2703e11) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->selfiespredict==0.0.post1.dev46+g2703e11) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->selfiespredict==0.0.post1.dev46+g2703e11) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown->selfiespredict==0.0.post1.dev46+g2703e11) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (3.1.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py->selfiespredict==0.0.post1.dev46+g2703e11) (2.0.1)\n",
            "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, selfies, rdkit-pypi, OpenNMT-py, selfiespredict\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Running setup.py develop for selfiespredict\n",
            "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.29.0 rdkit-pypi-2021.9.2.1 selfies-2.0.0 selfiespredict-0.0.post1.dev46+g2703e11 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqVjnPUv7bo3",
        "outputId": "fe1a493d-da04-4864-8261-6e1f8b953e6a"
      },
      "source": [
        "%cd /content/drive/My Drive/github/selfiespredict/run/testmodel"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc9i3xwNq4ry",
        "outputId": "6bed1b0e-e544-42de-fab5-f591e1c5b1ab"
      },
      "source": [
        "%cd run/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel/run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_UKcfHqq68M",
        "outputId": "4779ed78-debb-451c-f5ac-5e9552797ecc"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW2B2Hgp8OXk",
        "outputId": "d4a0141b-0474-4a52-c1c5-24b538bde7e0"
      },
      "source": [
        "! onmt_build_vocab -config test_model_params.yaml -src_seq_length 3000 -tgt_seq_length 3000 -src_vocab_size 3000 -tgt_vocab_size 3000 "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-11-23 21:25:52,682 INFO] Counter vocab from 5000 samples.\n",
            "[2021-11-23 21:25:52,682 INFO] Build vocab on 5000 transformed examples/corpus.\n",
            "[2021-11-23 21:25:52,694 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-11-23 21:25:52,901 INFO] Counters src:141\n",
            "[2021-11-23 21:25:52,901 INFO] Counters tgt:51\n",
            "[2021-11-23 21:25:52,901 INFO] Counters after share:142\n",
            "[2021-11-23 21:25:52,902 WARNING] path ./run/example.vocab.src exists, may overwrite...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo9EYI2tBWXx",
        "outputId": "6798290b-e88b-48cb-ddf7-94ec2ece8087"
      },
      "source": [
        "%%writefile gpu_usage.sh\n",
        "#! /bin/bash\n",
        "#comment: run for 10 seconds, change it as per your use\n",
        "end=$((SECONDS+100))\n",
        "\n",
        "while [ $SECONDS -lt $end ]; do\n",
        "    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu >> gpu.log\n",
        "    #comment: or use below command and comment above using #\n",
        "    #nvidia-smi dmon -i 0 -s mu -d 1 -o TD >> gpu.log\n",
        "done"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gpu_usage.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsPAbFsLrhKW",
        "outputId": "0e5a5402-6d5e-48a9-afee-2d187cc40fe5"
      },
      "source": [
        "%cd run\n",
        "%ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel/run\n",
            "example.vocab.src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnSxTVU6rvqb",
        "outputId": "caad4793-be9f-4ae4-f6e0-00303d814fde"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9jKsfbZcpaO",
        "outputId": "12eb84b5-c0f3-4c27-9fea-4bd73d87b3f4"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu.log  gpu_usage.sh  \u001b[0m\u001b[01;34mrun\u001b[0m/  test_model_params.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEjv0LV2rTLx",
        "outputId": "2aca5f2a-d1ab-4a80-f7b8-d397494cc14d"
      },
      "source": [
        "!head -n 10 test_model_params.yaml"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# test_model_params.yaml\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: ./run/example\n",
            "## Where the vocab(s) will be written, should point to same in shared embeddings\n",
            "src_vocab: ./run/example.vocab.src\n",
            "tgt_vocab: ./run/example.vocab.tgt\n",
            "# Prevent overwriting existing files in the folder\n",
            "overwrite: False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjbQLW_-x-Ws",
        "outputId": "2f59228d-bad1-469e-f650-4a0d65f703ad"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-HR7izeBX-t",
        "outputId": "3debe14c-6871-49bc-e8d0-5918ad9b0d05"
      },
      "source": [
        "%%bash --bg\n",
        "\n",
        "bash gpu_usage.sh"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job # 4 in a separate thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEY0gqGmwD-H",
        "outputId": "eeda23d2-d315-48cc-9811-c1568ae69abf"
      },
      "source": [
        "! onmt_train -config run.yaml \\\n",
        "        -seed 42 -gpu_ranks 0  \\\n",
        "        -train_steps 250000 -param_init 0 \\\n",
        "        -param_init_glorot -max_generator_batches 32 \\\n",
        "        -batch_type tokens -batch_size 6144\\\n",
        "         -normalization tokens -max_grad_norm 0  -accum_count 4 \\\n",
        "        -optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_method noam  \\\n",
        "        -warmup_steps 8000 -learning_rate 2 -label_smoothing 0.0 \\\n",
        "        -layers 4 -rnn_size  384 -word_vec_size 384 \\\n",
        "        -encoder_type transformer -decoder_type transformer \\\n",
        "        -dropout 0.1 -position_encoding -share_embeddings  \\\n",
        "        -global_attention general -global_attention_function softmax \\\n",
        "        -self_attn_type scaled-dot -heads 8 -transformer_ff 2048 \\\n",
        "        -report_every 2500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-23 22:33:26,323 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-11-23 22:33:26,324 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-11-23 22:33:26,325 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-11-23 22:33:26,325 INFO] Parsed 2 corpora from -data.\n",
            "[2021-11-23 22:33:26,326 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-11-23 22:33:26,326 INFO] Loading vocab from text file...\n",
            "[2021-11-23 22:33:26,326 INFO] Loading src vocabulary from ./run/example.vocab.src\n",
            "[2021-11-23 22:33:26,327 INFO] Loaded src vocab has 142 tokens.\n",
            "[2021-11-23 22:33:26,327 INFO] Loading tgt vocabulary from ./run/example.vocab.src\n",
            "[2021-11-23 22:33:26,329 INFO] Loaded tgt vocab has 142 tokens.\n",
            "[2021-11-23 22:33:26,329 INFO] Building fields with vocab in counters...\n",
            "[2021-11-23 22:33:26,330 INFO]  * tgt vocab size: 146.\n",
            "[2021-11-23 22:33:26,330 INFO]  * src vocab size: 144.\n",
            "[2021-11-23 22:33:26,330 INFO]  * merging src and tgt vocab...\n",
            "[2021-11-23 22:33:26,330 INFO]  * merged vocab size: 146.\n",
            "[2021-11-23 22:33:26,331 INFO]  * src vocab size = 146\n",
            "[2021-11-23 22:33:26,331 INFO]  * tgt vocab size = 146\n",
            "[2021-11-23 22:33:26,332 INFO] Building model...\n",
            "[2021-11-23 22:33:29,155 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(146, 384, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(146, 384, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=146, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-11-23 22:33:29,157 INFO] encoder: 8729600\n",
            "[2021-11-23 22:33:29,157 INFO] decoder: 11098258\n",
            "[2021-11-23 22:33:29,157 INFO] * number of parameters: 19827858\n",
            "[2021-11-23 22:33:29,161 INFO] Starting training on GPU: [0]\n",
            "[2021-11-23 22:33:29,161 INFO] Start training loop and validate every 10000 steps...\n",
            "[2021-11-23 22:33:29,161 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-11-23 22:33:29,162 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj3Q-JuX3LTR"
      },
      "source": [
        "#8791/5579 tok/s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGH0EcgI-Br7",
        "outputId": "501f08bc-f6a4-432f-e13b-6acbeb955acf"
      },
      "source": [
        "! onmt_train -config test_model_params.yaml -report_every 5000 "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-23 21:40:50,377 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-11-23 21:40:50,378 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-11-23 21:40:50,378 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-11-23 21:40:50,379 INFO] Parsed 2 corpora from -data.\n",
            "[2021-11-23 21:40:50,380 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-11-23 21:40:50,380 INFO] Loading vocab from text file...\n",
            "[2021-11-23 21:40:50,380 INFO] Loading src vocabulary from ./run/example.vocab.src\n",
            "[2021-11-23 21:40:50,381 INFO] Loaded src vocab has 142 tokens.\n",
            "[2021-11-23 21:40:50,382 INFO] Loading tgt vocabulary from ./run/example.vocab.src\n",
            "[2021-11-23 21:40:50,383 INFO] Loaded tgt vocab has 142 tokens.\n",
            "[2021-11-23 21:40:50,383 INFO] Building fields with vocab in counters...\n",
            "[2021-11-23 21:40:50,383 INFO]  * tgt vocab size: 146.\n",
            "[2021-11-23 21:40:50,384 INFO]  * src vocab size: 144.\n",
            "[2021-11-23 21:40:50,384 INFO]  * merging src and tgt vocab...\n",
            "[2021-11-23 21:40:50,384 INFO]  * merged vocab size: 146.\n",
            "[2021-11-23 21:40:50,384 INFO]  * src vocab size = 146\n",
            "[2021-11-23 21:40:50,384 INFO]  * tgt vocab size = 146\n",
            "[2021-11-23 21:40:50,385 INFO] Building model...\n",
            "[2021-11-23 21:40:53,260 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(146, 384, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.3, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.3, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.3, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.3, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(146, 384, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=384, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=384, bias=True)\n",
            "          (layer_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.3, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_values): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (linear_query): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=384, out_features=384, bias=True)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=384, out_features=146, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-11-23 21:40:53,262 INFO] encoder: 8729600\n",
            "[2021-11-23 21:40:53,262 INFO] decoder: 11098258\n",
            "[2021-11-23 21:40:53,262 INFO] * number of parameters: 19827858\n",
            "[2021-11-23 21:40:53,266 INFO] Starting training on GPU: [0]\n",
            "[2021-11-23 21:40:53,266 INFO] Start training loop and validate every 5000 steps...\n",
            "[2021-11-23 21:40:53,266 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-11-23 21:40:53,266 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/train.py\", line 172, in main\n",
            "    train(opt)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/bin/train.py\", line 157, in train\n",
            "    train_process(opt, device_id=0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/train_single.py\", line 114, in main\n",
            "    valid_steps=opt.valid_steps)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/trainer.py\", line 244, in train\n",
            "    report_stats)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/trainer.py\", line 368, in _gradient_accumulation\n",
            "    with_align=self.with_align)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/models/model.py\", line 69, in forward\n",
            "    with_align=with_align)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/decoders/transformer.py\", line 473, in forward\n",
            "    with_align=with_align,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/decoders/transformer.py\", line 98, in forward\n",
            "    output, attns = self._forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/decoders/transformer.py\", line 263, in _forward\n",
            "    inputs_norm, dec_mask, layer_cache, step\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/decoders/transformer.py\", line 150, in _forward_self_attn\n",
            "    attn_type=\"self\",\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/onmt/modules/multi_headed_attn.py\", line 203, in forward\n",
            "    drop_attn = self.dropout(attn)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
            "    return F.dropout(input, self.p, self.training, self.inplace)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1169, in dropout\n",
            "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00p1NBIY9ann",
        "outputId": "46a5cf6c-0df6-4547-aa51-016010a835b3"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/run/testmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWnz14aDFN-E",
        "outputId": "41a95d7d-aa6c-4555-d809-3cac8424d2f8"
      },
      "source": [
        "%ls testmodel"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_model_params.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Md6fx4pDJji"
      },
      "source": [
        "cleaner = Data_Cleaner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oDdeAsJDJlR",
        "outputId": "580e03d1-b05a-4f52-b64f-eef7ce99588f"
      },
      "source": [
        "cleaner.import_data(\"USPTO_STEREO\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r3_7WMEor7-CgN34Foj-ET-uFco0fURU\n",
            "To: /content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/src-train.txt\n",
            "100%|██████████| 137M/137M [00:01<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/tgt-train.txt exists, skip downloading\n",
            "/content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/src-val.txt exists, skip downloading\n",
            "/content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/tgt-val.txt exists, skip downloading\n",
            "/content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/src-test.txt exists, skip downloading\n",
            "/content/drive/My Drive/github/selfiespredict/data/raw_data/USPTO_STEREO/tgt-test.txt exists, skip downloading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "1xC-Wmc-J2D7",
        "outputId": "06aee19a-090a-43a3-8786-2d5e60300559"
      },
      "source": [
        "cleaner.gen_txt(\"USPTO_480k\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5023f06dbc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USPTO_480k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/My Drive/github/selfiespredict/src/selfiespredict/data/load_data.py\u001b[0m in \u001b[0;36mgen_txt\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata2selfie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_selfies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/github/selfiespredict/src/selfiespredict/data/load_data.py\u001b[0m in \u001b[0;36mdata2selfie\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconverts\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mentries\u001b[0m \u001b[0mto\u001b[0m \u001b[0mSELFIE\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SELFIE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/github/selfiespredict/src/selfiespredict/data/load_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconverts\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mentries\u001b[0m \u001b[0mto\u001b[0m \u001b[0mSELFIE\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SELFIE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selfies/encoder.py\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(smiles, strict)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEncoderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkekulize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"kekulization failed\\n\\tSMILES: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEncoderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selfies/mol_graph.py\u001b[0m in \u001b[0;36mkekulize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mpruned_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_to_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mmatching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_perfect_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruned_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatching\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selfies/utils/matching_utils.py\u001b[0m in \u001b[0;36mfind_perfect_matching\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# start with a maximal matching for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmatching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_greedy_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0munmatched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/selfies/utils/matching_utils.py\u001b[0m in \u001b[0;36m_greedy_matching\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mfree_degrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatching\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfree_degrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mheapq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheappush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_pqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfree_degrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMT9DSxZKX_f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSV5pJPA6Pk1"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpAp6mMm5xFo",
        "outputId": "2e11d83d-3256-4778-f125-f50184c5c0f3"
      },
      "source": [
        "%mkdir pip_fix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pip_fix’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-lSEi9EHZRR",
        "outputId": "688fb819-48bc-4fb4-b1bb-c143ac6af590"
      },
      "source": [
        "username = ''\n",
        "repository = ''\n",
        "git_token = ''\n",
        "!git clone https://{git_token}@github.com/{username}/{repository}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'selfiespredict'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 195 (delta 82), reused 170 (delta 65), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (195/195), 22.60 MiB | 4.69 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "Checking out files: 100% (56/56), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCwLFjKwMLCp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}